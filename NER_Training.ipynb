{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER-Training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "F609YhnbUPnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Installing dependencies\n",
        "# Uncomment and run this cell once in the beginning\n",
        "# ! pip install spacy google-cloud-vision pandas tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejURjAH0Zh2C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing libraries\n",
        "import random\n",
        "import re\n",
        "import spacy\n",
        "import os \n",
        "import json\n",
        "from google.cloud import vision\n",
        "import io\n",
        "import pandas as pd\n",
        "from tqdm import tqdm_notebook as tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2evxvafEJpZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# JSON formatting functions\n",
        "def convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n",
        "    try:\n",
        "        training_data = []\n",
        "        lines=[]\n",
        "        with open(dataturks_JSON_FilePath, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        for line in lines:\n",
        "            data = json.loads(line)\n",
        "            text = data['content']\n",
        "            entities = []\n",
        "            data_annotations = data['annotation']\n",
        "            if data_annotations is not None:\n",
        "                for annotation in data_annotations:\n",
        "                    #only a single point in text annotation.\n",
        "                    point = annotation['points'][0]\n",
        "                    labels = annotation['label']\n",
        "                    # handle both list of labels or a single label.\n",
        "                    if not isinstance(labels, list):\n",
        "                        labels = [labels]\n",
        "\n",
        "                    for label in labels:\n",
        "                        point_start = point['start']\n",
        "                        point_end = point['end']\n",
        "                        point_text = point['text']\n",
        "                        \n",
        "                        lstrip_diff = len(point_text) - len(point_text.lstrip())\n",
        "                        rstrip_diff = len(point_text) - len(point_text.rstrip())\n",
        "                        if lstrip_diff != 0:\n",
        "                            point_start = point_start + lstrip_diff\n",
        "                        if rstrip_diff != 0:\n",
        "                            point_end = point_end - rstrip_diff\n",
        "                        entities.append((point_start, point_end + 1 , label))\n",
        "            training_data.append((text, {\"entities\" : entities}))\n",
        "        return training_data\n",
        "    except Exception as e:\n",
        "        logging.exception(\"Unable to process \" + dataturks_JSON_FilePath + \"\\n\" + \"error = \" + str(e))\n",
        "        return None\n",
        "\n",
        "def trim_entity_spans(data: list) -> list:\n",
        "    \"\"\"Removes leading and trailing white spaces from entity spans.\n",
        "\n",
        "    Args:\n",
        "        data (list): The data to be cleaned in spaCy JSON format.\n",
        "\n",
        "    Returns:\n",
        "        list: The cleaned data.\n",
        "    \"\"\"\n",
        "    invalid_span_tokens = re.compile(r'\\s')\n",
        "\n",
        "    cleaned_data = []\n",
        "    for text, annotations in data:\n",
        "        entities = annotations['entities']\n",
        "        valid_entities = []\n",
        "        for start, end, label in entities:\n",
        "            valid_start = start\n",
        "            valid_end = end\n",
        "            while valid_start < len(text) and invalid_span_tokens.match(\n",
        "                    text[valid_start]):\n",
        "                valid_start += 1\n",
        "            while valid_end > 1 and invalid_span_tokens.match(\n",
        "                    text[valid_end - 1]):\n",
        "                valid_end -= 1\n",
        "            valid_entities.append([valid_start, valid_end, label])\n",
        "        cleaned_data.append([text, {'entities': valid_entities}])\n",
        "    return cleaned_data        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aflSNJEyaU-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################### Train Spacy NER.###########\n",
        "def train_spacy():\n",
        "\n",
        "    TRAIN_DATA = trim_entity_spans(convert_dataturks_to_spacy(\"SpacyData.json\")) #train data\n",
        "    nlp = spacy.blank('en')  # create blank Language class\n",
        "    # create the built-in pipeline components and add them to the pipeline\n",
        "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
        "    if 'ner' not in nlp.pipe_names:\n",
        "        ner = nlp.create_pipe('ner')\n",
        "        nlp.add_pipe(ner, last=True)\n",
        "       \n",
        "\n",
        "    # add labels\n",
        "    for _, annotations in TRAIN_DATA:\n",
        "         for ent in annotations.get('entities'):\n",
        "            ner.add_label(ent[2])\n",
        "\n",
        "    # get names of other pipes to disable them during training\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "        optimizer = nlp.begin_training()\n",
        "        for itn in range(20):\n",
        "            print(\"Starting iteration \" + str(itn))\n",
        "            random.shuffle(TRAIN_DATA)\n",
        "            losses = {}\n",
        "            for text, annotations in TRAIN_DATA:\n",
        "                nlp.update(\n",
        "                    [text],  # batch of texts\n",
        "                    [annotations],  # batch of annotations\n",
        "                    drop=0.3,  # dropout - make it harder to memorise data\n",
        "                    sgd=optimizer,  # callable to update weights\n",
        "                    losses=losses)\n",
        "            print(losses)\n",
        "            # Saving model to disk\n",
        "            nlp.to_disk(\"NER\")\n",
        "    return nlp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10u8kZUBdZuw",
        "colab_type": "code",
        "outputId": "853fadcc-0a53-4945-b03a-9675b2cb9943",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "# Training the NER model for 20 Epochs\n",
        "nlp = train_spacy()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting iteration 0\n",
            "{'ner': 1389.4218910541458}\n",
            "Starting iteration 1\n",
            "{'ner': 302.9351514141446}\n",
            "Starting iteration 2\n",
            "{'ner': 367.38948576275675}\n",
            "Starting iteration 3\n",
            "{'ner': 147.95892109721365}\n",
            "Starting iteration 4\n",
            "{'ner': 164.62000559092425}\n",
            "Starting iteration 5\n",
            "{'ner': 167.2583772462929}\n",
            "Starting iteration 6\n",
            "{'ner': 61.91851513966879}\n",
            "Starting iteration 7\n",
            "{'ner': 38.986973023105826}\n",
            "Starting iteration 8\n",
            "{'ner': 47.51857286418727}\n",
            "Starting iteration 9\n",
            "{'ner': 31.212716692883966}\n",
            "Starting iteration 10\n",
            "{'ner': 33.38858660239155}\n",
            "Starting iteration 11\n",
            "{'ner': 20.69545877822283}\n",
            "Starting iteration 12\n",
            "{'ner': 30.322275450144378}\n",
            "Starting iteration 13\n",
            "{'ner': 21.912833313412005}\n",
            "Starting iteration 14\n",
            "{'ner': 10.607650553790785}\n",
            "Starting iteration 15\n",
            "{'ner': 25.74186658124544}\n",
            "Starting iteration 16\n",
            "{'ner': 16.49051592265769}\n",
            "Starting iteration 17\n",
            "{'ner': 20.084303738077946}\n",
            "Starting iteration 18\n",
            "{'ner': 11.082623067535678}\n",
            "Starting iteration 19\n",
            "{'ner': 9.83011691064422}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sd5vBvObncJg",
        "colab_type": "code",
        "outputId": "d1e33f35-6eff-41dd-f59a-139344f0a239",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# Zipping model folder\n",
        "! zip -r NER.zip NER/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: NER/ (stored 0%)\n",
            "  adding: NER/vocab/ (stored 0%)\n",
            "  adding: NER/vocab/key2row (stored 0%)\n",
            "  adding: NER/vocab/lexemes.bin (deflated 69%)\n",
            "  adding: NER/vocab/vectors (deflated 45%)\n",
            "  adding: NER/vocab/strings.json (deflated 68%)\n",
            "  adding: NER/meta.json (deflated 41%)\n",
            "  adding: NER/tokenizer (deflated 83%)\n",
            "  adding: NER/ner/ (stored 0%)\n",
            "  adding: NER/ner/cfg (deflated 47%)\n",
            "  adding: NER/ner/model (deflated 7%)\n",
            "  adding: NER/ner/moves (deflated 74%)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}